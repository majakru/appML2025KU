{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8653ca8",
   "metadata": {},
   "source": [
    "# Supervised Classification Analysis of Single-cell RNA-seq Data\n",
    "\n",
    "**Goal:** Train a classifier (LogisticRegression & LightGBM) for each cell in pancreas_islet.h5ad based on its disease label, and output prediction results and evaluation report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import nbformat as nbf\n",
    "import os\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from itertools import cycle\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import warnings\n",
    "import matplotlib\n",
    "import joblib\n",
    "import shap\n",
    "import umap\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set matplotlib backend to avoid font issues\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "# Configure font settings\n",
    "plt.rcParams['font.family'] = ['Arial', 'DejaVu Sans', 'Liberation Sans', 'sans-serif']\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.max_open_warning'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f75197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "if os.path.exists(\"preprocessed_pancreas_islet.h5ad\"):\n",
    "    print(\"Loading preprocessed data...\")\n",
    "    adata = sc.read_h5ad(\"preprocessed_pancreas_islet.h5ad\")\n",
    "else:\n",
    "    print(\"Loading raw data and preprocessing...\")\n",
    "    adata = sc.read_h5ad(\"pancreas_islet.h5ad\")\n",
    "    sc.pp.filter_genes(adata, min_cells=20)\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=3000)\n",
    "    adata = adata[:, adata.var.highly_variable]\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.pp.scale(adata, max_value=10)\n",
    "    adata.write_h5ad(\"preprocessed_pancreas_islet.h5ad\")\n",
    "    print(\"Preprocessing completed, data saved!\")\n",
    "\n",
    "# Print basic data information\n",
    "print(f\"Data shape: {adata.shape}\")\n",
    "print(f\"Available annotations: {list(adata.obs.columns)}\")\n",
    "\n",
    "# Verify disease label exists\n",
    "if 'disease' not in adata.obs.columns:\n",
    "    raise ValueError(\"Data does not contain 'disease' label, please check the dataset\")\n",
    "\n",
    "# Find donor identifier columns\n",
    "donor_columns = []\n",
    "for col in adata.obs.columns:\n",
    "    if 'donor' in col.lower() or 'individual' in col.lower() or 'subject' in col.lower():\n",
    "        donor_columns.append(col)\n",
    "\n",
    "if donor_columns:\n",
    "    print(f\"Found possible donor-related columns: {donor_columns}\")\n",
    "    donor_id_column = donor_columns[0]\n",
    "else:\n",
    "    # If no clear donor column is found, try to find other possible grouping columns\n",
    "    possible_columns = [col for col in adata.obs.columns \n",
    "                        if any(k in col.lower() for k in ['batch', 'sample', 'id', 'group'])]\n",
    "    if possible_columns:\n",
    "        donor_id_column = possible_columns[0]\n",
    "    else:\n",
    "        # If still no suitable grouping column is found, create random grouping\n",
    "        print(\"No suitable grouping column found, using random grouping\")\n",
    "        adata.obs['random_group'] = np.random.randint(0, 10, size=adata.shape[0])\n",
    "        donor_id_column = 'random_group'\n",
    "        \n",
    "print(f\"Using '{donor_id_column}' as donor identifier\")\n",
    "\n",
    "# Print disease category statistics\n",
    "print(\"\\nDisease category statistics:\")\n",
    "print(adata.obs['disease'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ab546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "print(\"\\nPreprocessing data...\")\n",
    "\n",
    "# Preprocess data according to requirements\n",
    "with tqdm(total=5, desc=\"Progress of preprocessing\") as pbar:\n",
    "    # Filter genes\n",
    "    sc.pp.filter_genes(adata, min_cells=20)\n",
    "    pbar.update(1)\n",
    "    \n",
    "    # Select highly variable genes\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=3000)\n",
    "    adata = adata[:, adata.var.highly_variable]\n",
    "    pbar.update(1)\n",
    "    \n",
    "    # Normalize\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    pbar.update(1)\n",
    "    \n",
    "    # Logarithmic transformation\n",
    "    sc.pp.log1p(adata)\n",
    "    pbar.update(1)\n",
    "    \n",
    "    # Scaling\n",
    "    sc.pp.scale(adata, max_value=10)\n",
    "    pbar.update(1)\n",
    "\n",
    "print(f\"Shape of preprocessed data: {adata.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c1a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data based on donor_id (80%/20%)\n",
    "print(f\"\\nSplitting data based on {donor_id_column}...\")\n",
    "donors = adata.obs[donor_id_column].unique()\n",
    "train_donors, test_donors = train_test_split(donors, test_size=0.2, random_state=42, stratify=donors)\n",
    "\n",
    "# Create training set and test set\n",
    "train_mask = adata.obs[donor_id_column].isin(train_donors)\n",
    "test_mask = adata.obs[donor_id_column].isin(test_donors)\n",
    "\n",
    "train_adata = adata[train_mask]\n",
    "test_adata = adata[test_mask]\n",
    "\n",
    "print(f\"Number of cells in training set: {train_adata.shape[0]}\")\n",
    "print(f\"Number of cells in test set: {test_adata.shape[0]}\")\n",
    "\n",
    "# Extract feature matrix and labels\n",
    "X_train = train_adata.X\n",
    "y_train = train_adata.obs['disease'].astype(str).values  # Convert to string array\n",
    "X_test = test_adata.X\n",
    "y_test = test_adata.obs['disease'].astype(str).values    # Convert to string array\n",
    "\n",
    "# Ensure X is a dense matrix (if sparse)\n",
    "if hasattr(X_train, 'toarray'):\n",
    "    X_train = X_train.toarray()\n",
    "    X_test = X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64019712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and load pre-trained models\n",
    "print(\"Checking if pre-trained models exist...\")\n",
    "\n",
    "if os.path.exists('lightgbm_model.txt') and os.path.exists('logistic_regression_model.pkl'):\n",
    "    print(\"Pre-trained models found, loading...\")\n",
    "    \n",
    "    # Load LightGBM model\n",
    "    gbm = lgb.Booster(model_file='lightgbm_model.txt')\n",
    "    print(\"LightGBM model loaded\")\n",
    "    \n",
    "    # Load Logistic Regression model and preprocessors\n",
    "    lr = joblib.load('logistic_regression_model.pkl')\n",
    "    label_encoder = joblib.load('label_encoder.pkl')\n",
    "    imputer = joblib.load('imputer.pkl')\n",
    "    print(\"Logistic Regression model loaded\")\n",
    "    \n",
    "    # Get original classes\n",
    "    original_classes = label_encoder.classes_\n",
    "    print(f\"Loaded class mapping: {dict(zip(original_classes, range(len(original_classes))))}\")\n",
    "    \n",
    "    # Apply saved preprocessors\n",
    "    X_train = imputer.transform(X_train)\n",
    "    X_test = imputer.transform(X_test)\n",
    "    \n",
    "    # Perform label encoding\n",
    "    y_train_encoded = label_encoder.transform(y_train)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "    \n",
    "    print(\"Models loaded, ready for prediction and evaluation\")\n",
    "else:\n",
    "    print(\"Pre-trained models not found, need to retrain...\")\n",
    "    # Continue with training code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9fb10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression model\n",
    "print(\"\\nTraining Logistic Regression model...\")\n",
    "print(f\"Size of training data: {X_train.shape}\")\n",
    "\n",
    "# If data is too large, can take a sample for training\n",
    "if X_train.shape[0] > 10000:\n",
    "    from sklearn.utils import resample\n",
    "    X_train_sample, y_train_sample = resample(\n",
    "        X_train, y_train, n_samples=10000, random_state=42, stratify=y_train\n",
    "    )\n",
    "    print(f\"Data size is large, using 10000 samples for training...\")\n",
    "    print(f\"Size of sampled training data: {X_train_sample.shape}\")\n",
    "else:\n",
    "    X_train_sample, y_train_sample = X_train, y_train\n",
    "\n",
    "# Check and fill NaN, to prevent Logistic Regression from crashing\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "X_train_sample = imputer.fit_transform(X_train_sample)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "with tqdm(total=100, desc=\"Training progress\") as pbar:\n",
    "    lr = LogisticRegression(multi_class=\"multinomial\", solver=\"saga\", max_iter=200, n_jobs=-1, random_state=42)\n",
    "    pbar.update(10)\n",
    "    lr.fit(X_train_sample, y_train_sample)\n",
    "    pbar.update(90)\n",
    "end_time = time.time()\n",
    "print(f\"Training completed! Time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Predict\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "y_pred_proba_lr = lr.predict_proba(X_test)\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "print(\"\\nEvaluation of Logistic Regression model:\")\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "lr_f1_macro = f1_score(y_test, y_pred_lr, average='macro')\n",
    "print(f\"Accuracy: {lr_accuracy:.4f}\")\n",
    "print(f\"Macro-F1: {lr_f1_macro:.4f}\")\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62eed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM model\n",
    "print(\"\\nTraining LightGBM model...\")\n",
    "\n",
    "# Get class labels\n",
    "classes = np.unique(y_train)\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Create dataset\n",
    "lgb_train = lgb.Dataset(X_train, y_train)  # Use full dataset\n",
    "lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "# Set parameters\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': num_classes,\n",
    "    'metric': 'multi_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'num_threads': -1,  # Use all CPUs\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Train model\n",
    "print(\"Starting LightGBM training...\")\n",
    "gbm = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    num_boost_round=100,\n",
    "    valid_sets=[lgb_train, lgb_test]\n",
    ")\n",
    "\n",
    "# Save LightGBM model\n",
    "print(\"Saving LightGBM model...\")\n",
    "gbm.save_model('lightgbm_model.txt')\n",
    "print(\" LightGBM model saved as 'lightgbm_model.txt'\")\n",
    "\n",
    "# Predict\n",
    "y_pred_proba_lgb = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "y_pred_lgb = np.argmax(y_pred_proba_lgb, axis=1)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "y_pred_lgb = label_encoder.inverse_transform(y_pred_lgb)\n",
    "\n",
    "# Evaluate LightGBM\n",
    "print(\"\\nEvaluation of LightGBM model:\")\n",
    "lgb_accuracy = accuracy_score(y_test, y_pred_lgb)\n",
    "lgb_f1_macro = f1_score(y_test, y_pred_lgb, average='macro')\n",
    "print(f\"Accuracy: {lgb_accuracy:.4f}\")\n",
    "print(f\"Macro-F1: {lgb_f1_macro:.4f}\")\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred_lgb))\n",
    "\n",
    "# Compare two models\n",
    "print(\"\\nModel comparison:\")\n",
    "print(f\"Logistic Regression - Accuracy: {lr_accuracy:.4f}, Macro-F1: {lr_f1_macro:.4f}\")\n",
    "print(f\"LightGBM - Accuracy: {lgb_accuracy:.4f}, Macro-F1: {lgb_f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6f0c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save prediction results CSV\n",
    "print(\"\\nCreating prediction results...\")\n",
    "cell_indices = range(len(test_adata.obs.index))\n",
    "predictions_df = pd.DataFrame()\n",
    "predictions_df['cell_index'] = cell_indices\n",
    "\n",
    "# Ensure class names are consistent\n",
    "required_classes = ['normal', 'type 1 diabetes mellitus', 'type 2 diabetes mellitus', 'endocrine pancreas disorder']\n",
    "\n",
    "# Check if all required classes exist\n",
    "missing_classes = set(required_classes) - set(original_classes)\n",
    "if missing_classes:\n",
    "    print(f\"Warning: Some required classes are missing in data: {missing_classes}\")\n",
    "    print(f\"Actual classes: {original_classes}\")\n",
    "\n",
    "# Add probability columns for each class (in the specified order)\n",
    "for cls in required_classes:\n",
    "    if cls in original_classes:\n",
    "        cls_idx = np.where(original_classes == cls)[0][0]\n",
    "        predictions_df[f'p_{cls}'] = lr.predict_proba(X_test)[:, cls_idx]\n",
    "    else:\n",
    "        # If class doesn't exist, fill with 0\n",
    "        predictions_df[f'p_{cls}'] = 0\n",
    "\n",
    "# Save prediction results\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "print(\"Prediction results saved to 'predictions.csv'\")\n",
    "\n",
    "# Create confusion matrix plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, lr.predict(X_test))\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix for Logistic Regression')\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics.png')\n",
    "print(\"Confusion matrix plot saved to 'metrics.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644470f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP Visualization\n",
    "import umap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create UMAP embeddings\n",
    "print(\"Creating UMAP embeddings...\")\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "X_test_umap = reducer.fit_transform(X_test)\n",
    "\n",
    "# Prepare predictions\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "y_pred_lgb_labels = label_encoder.inverse_transform(np.argmax(gbm.predict(X_test), axis=1))\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# True labels\n",
    "for i, disease in enumerate(np.unique(y_test)):\n",
    "    mask = y_test == disease\n",
    "    axes[0].scatter(X_test_umap[mask, 0], X_test_umap[mask, 1], \n",
    "                   label=disease, alpha=0.7, s=10)\n",
    "axes[0].set_title('True Labels')\n",
    "axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# LightGBM predictions\n",
    "for i, disease in enumerate(np.unique(y_test)):\n",
    "    mask = y_pred_lgb_labels == disease\n",
    "    axes[1].scatter(X_test_umap[mask, 0], X_test_umap[mask, 1], \n",
    "                   label=disease, alpha=0.7, s=10)\n",
    "axes[1].set_title('LightGBM Predictions')\n",
    "axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Logistic Regression predictions\n",
    "for i, disease in enumerate(np.unique(y_test)):\n",
    "    mask = y_pred_lr == disease\n",
    "    axes[2].scatter(X_test_umap[mask, 0], X_test_umap[mask, 1], \n",
    "                   label=disease, alpha=0.7, s=10)\n",
    "axes[2].set_title('Logistic Regression Predictions')\n",
    "axes[2].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('umap_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"UMAP visualization saved to 'umap_visualization.png'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:appml2025]",
   "language": "python",
   "name": "conda-env-appml2025-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
